{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff58c8da-1eb2-4376-9ad5-0e152bde4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USER_FLAG = \"--user\"\n",
    "#!pip3 install {USER_FLAG} kfp --upgrade\n",
    "#!pip3 install {USER_FLAG} google_cloud_pipeline_components --upgrade\n",
    "#!pip3 install {USER_FLAG} 'apache-beam[gcp]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb455c6-8d1a-4b63-b016-b2f19d20b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48e25f7-c840-441b-9b86-5ff03542baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.25.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"from google.cloud import aiplatform; print('aiplatform SDK version: {}'.format(aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a5b62-e87b-437b-bda0-6481532e8a07",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07faf00e-623c-40d8-8fb8-99513549d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'aiplatform' from 'google_cloud_pipeline_components' (/home/jupyter/.local/lib/python3.7/site-packages/google_cloud_pipeline_components/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/1427607336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle_cloud_pipeline_components\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiplatform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgcc_aip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'aiplatform' from 'google_cloud_pipeline_components' (/home/jupyter/.local/lib/python3.7/site-packages/google_cloud_pipeline_components/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (pipeline,\n",
    "                        Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        Markdown)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f4007-f524-414a-b403-8989e7b6a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_root = 'gs://bucket_pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578d639-b122-409f-b17b-c5950cca7ef0",
   "metadata": {},
   "source": [
    "# 1.0 Data Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a686c-b534-4fc9-b167-f3d19cb0a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-bigquery\",\"db-dtypes\", \"pandas\"],\n",
    "          base_image=\"python:3.10.6\",\n",
    "          output_component_file=\"captura_dados.yaml\")\n",
    "def captura_dados():\n",
    "    import logging\n",
    "\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    PROJECT_ID = \"gcp-vertex\"\n",
    "    DATASET_ID = \"gcp_bq\"\n",
    "    TABLE_RAW_ID = \"dados_ecommerce_raw\"\n",
    "    TABLE_ID = \"ecommerce_cds\"\n",
    "    \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    query = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE\n",
    "               `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` (invoice_no STRING,\n",
    "                stock_code STRING,\n",
    "                description STRING,\n",
    "                quantity INT64,\n",
    "                invoice_date DATE,\n",
    "                unit_price FLOAT64,\n",
    "                customer_id FLOAT64,\n",
    "                country STRING)\n",
    "            PARTITION BY\n",
    "              invoice_date AS (\n",
    "              WITH\n",
    "                not_nulls AS (\n",
    "                SELECT\n",
    "                  *\n",
    "                FROM\n",
    "                  `{PROJECT_ID}.{DATASET_ID}.{TABLE_RAW_ID}`\n",
    "                WHERE\n",
    "                  invoice_date <= CURRENT_DATE()\n",
    "                  AND customer_id IS NOT NULL\n",
    "                  AND description IS NOT NULL),\n",
    "                filtering_features AS (\n",
    "                SELECT\n",
    "                  *\n",
    "                FROM\n",
    "                  not_nulls\n",
    "                WHERE\n",
    "                  unit_price >= 0.04\n",
    "                  AND country NOT IN ('European Community',\n",
    "                    'Unspecified')\n",
    "                  AND stock_code NOT IN ('POST',\n",
    "                    'D',\n",
    "                    'DOT',\n",
    "                    'M',\n",
    "                    'S',\n",
    "                    'AMAZONFEE',\n",
    "                    'm',\n",
    "                    'DCGSSBOY',\n",
    "                    'DCGSSGIRL',\n",
    "                    'PADS',\n",
    "                    'B',\n",
    "                    'CRUK')\n",
    "                  AND customer_id != 16446)\n",
    "              SELECT\n",
    "                *\n",
    "              FROM\n",
    "                filtering_features);\n",
    "    \"\"\"\n",
    "    \n",
    "    run_bq_query(query, project_name=PROJECT_ID)\n",
    "    logging.info(f'Tabela criada: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2218a3-58c2-49f4-a3eb-387baf5d9ed3",
   "metadata": {},
   "source": [
    "# 2.0 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ab1ef-2eb4-44f9-975a-5e0e878b8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db-dtypes\", \"pandas-gbq\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"data_preparation_ecommerce.yaml\")\n",
    "def data_preparation():\n",
    "    import os\n",
    "    import logging\n",
    "    from typing import Tuple\n",
    "\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID = 'gcpproject'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_RAW_ID = 'dados_ecommerce_raw'\n",
    "    TABLE_ID = 'ecommerce_cds'\n",
    "    TABLE_FILTERED_TEMP_ID = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID = 'temp_data_returns'\n",
    "    PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "\n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_columns.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "    def column_to_int(dataframe: pd.DataFrame, column_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo inteiro.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe[column_name] = dataframe[column_name].astype(int)\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "    \n",
    "    def column_to_date(dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo data.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "            date_format (str, opcional): O formato de data personalizado. Se nenhum formato for especificado, o pandas usará o padrão 'YYYY-MM-DD'.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format=date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "\n",
    "\n",
    "    def change_column_type(dataframe_raw: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Changes the data type of a given column in a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        column_to_int(dataframe_raw, 'customer_id')\n",
    "        column_to_date(dataframe_raw, 'invoice_date')\n",
    "\n",
    "    def filtering_features(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Filters and preprocesses the input dataframe.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw: A pandas DataFrame containing raw sales data.\n",
    "\n",
    "        Returns:\n",
    "            Three pandas DataFrames containing the filtered returns and purchases data, and the filtered main data.\n",
    "        \"\"\"\n",
    "        # Filter returns and purchases data\n",
    "        df_returns = dataframe_raw.loc[dataframe_raw['quantity'] < 0, ['customer_id', \n",
    "                                                                       'quantity']]\n",
    "        df_purchases = dataframe_raw.loc[dataframe_raw['quantity'] >= 0, :]\n",
    "\n",
    "        # Filter main data\n",
    "        df_filtered = keep_features(dataframe_raw, ['invoice_no', 'stock_code', 'quantity',\n",
    "                                                    'invoice_date', 'unit_price', \n",
    "                                                    'customer_id', 'country'])\n",
    "        return df_filtered, df_purchases, df_returns\n",
    "\n",
    "    def run_data_preparation(dataframe_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocesses the input dataframe by performing column type conversion and filtering features.\n",
    "\n",
    "        Args:\n",
    "            dataframe_raw (pd.DataFrame): A pandas DataFrame containing raw sales data.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three pandas DataFrames: df_filtered, df_purchases, and df_returns.\n",
    "            - df_filtered: A DataFrame containing the filtered main data.\n",
    "            - df_purchases: A DataFrame containing the filtered purchases data.\n",
    "            - df_returns: A DataFrame containing the filtered returns data.\n",
    "        \"\"\"\n",
    "        change_column_type(dataframe_raw)\n",
    "        return filtering_features(dataframe_raw)\n",
    "    \n",
    "    query_sql = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "                    WHERE invoice_date <= CURRENT_DATE \"\"\"\n",
    "    \n",
    "    data = pd.read_gbq(query=query_sql, \n",
    "                         project_id=PROJECT_NUMBER) \n",
    "    logging.info(f'Tabela carregada: `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`')\n",
    "    \n",
    "    df_filtered, df_purchases, df_returns = run_data_preparation(data)\n",
    "    \n",
    "    pandas_gbq.to_gbq(df_filtered, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_FILTERED_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    pandas_gbq.to_gbq(df_purchases, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_PURCHASES_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    pandas_gbq.to_gbq(df_returns, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_RETURNS_TEMP_ID}', project_id=PROJECT_NUMBER, if_exists='replace')\n",
    "    \n",
    "    logging.info(f'Tabelas criadas no BigQuery: {TABLE_FILTERED_TEMP_ID} e {TABLE_PURCHASES_TEMP_ID} e {TABLE_RETURNS_TEMP_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562633d0-0b7b-4a0c-b251-b7e3ffe23651",
   "metadata": {},
   "source": [
    "# 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe5f23-9c33-4614-9725-cbb3d896824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db-dtypes\", \"pandas-gbq\", \"google-cloud\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"feature_engineering_ecommerce.yaml\")\n",
    "def feature_engineering():\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import os\n",
    "    import logging\n",
    "    from functools import reduce\n",
    "    from typing import Union\n",
    "    \n",
    "    PROJECT_ID = 'gcpproject'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_ID = 'dados_engenharia_features'\n",
    "    TABLE_RAW_ID = 'dados_ecommerce_raw'\n",
    "    TABLE_FILTERED_TEMP_ID = 'temp_data_filtered'\n",
    "    TABLE_PURCHASES_TEMP_ID = 'temp_data_purchases'\n",
    "    TABLE_RETURNS_TEMP_ID = 'temp_data_returns'\n",
    "    PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    def column_to_date(dataframe: pd.DataFrame, column_name: str, date_format: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Converte a coluna especificada em um dataframe para o tipo data.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O dataframe a ser processado.\n",
    "            column_name (str): O nome da coluna a ser convertida.\n",
    "            date_format (str, opcional): O formato de data personalizado. Se nenhum formato for especificado, o pandas usará o padrão 'YYYY-MM-DD'.\n",
    "\n",
    "        Returns:\n",
    "            bool: True se a conversão foi bem sucedida, False caso contrário.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if date_format:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name], format=date_format)\n",
    "            else:\n",
    "                dataframe[column_name] = pd.to_datetime(dataframe[column_name])\n",
    "        except (ValueError, TypeError):\n",
    "            # Lidar com valores ausentes e conversões inválidas\n",
    "            return False\n",
    "\n",
    "        # Retorna True se a conversão foi bem sucedida\n",
    "        return True\n",
    "    def keep_features(dataframe: pd.DataFrame, keep_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com as colunas especificadas em keep_columns.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): O DataFrame a ser processado.\n",
    "            keep_columns (list): A lista de nomes de colunas a serem mantidas no DataFrame resultante.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante com apenas as colunas especificadas em keep_columns.\n",
    "        \"\"\"\n",
    "        return dataframe[keep_columns]\n",
    "    \n",
    "    def calculate_gross_revenue(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a receita bruta de cada cliente com base nas colunas 'Quantity' e 'UnitPrice' e retorna\n",
    "        um DataFrame com as colunas 'CustomerID' e 'gross_revenue'.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): O DataFrame das compras contendo as colunas 'CustomerID', 'Quantity' e 'UnitPrice'.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: O DataFrame resultante contendo as colunas 'CustomerID' e 'gross_revenue'.\n",
    "        \"\"\"\n",
    "        # Verifica se as colunas necessárias estão presentes no DataFrame de entrada\n",
    "        required_columns = {'customer_id', 'quantity', 'UnitPrice'}\n",
    "        missing_columns = required_columns - set(dataframe_purchases.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"O DataFrame de entrada está faltando as seguintes colunas: {missing_columns}\")\n",
    "\n",
    "        # Calcula a receita bruta e agrupa por CustomerID\n",
    "        dataframe_purchases.loc[:, 'gross_revenue'] = dataframe_purchases.loc[:, 'quantity'] * dataframe_purchases.loc[:, 'UnitPrice']\n",
    "        grouped_df = dataframe_purchases.groupby('customer_id').agg({'gross_revenue': 'sum'}).reset_index()\n",
    "\n",
    "        return grouped_df\n",
    "\n",
    "    def create_recency(dataframe_purchases: pd.DataFrame, dataframe_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a recência da última compra para cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "            dataframe_filtered (pd.DataFrame): DataFrame filtrado apenas com as informações dos clientes que desejamos calcular a recência.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'CustomerID' e 'recency_days', indicando a recência em dias da última compra para cada cliente.\n",
    "\n",
    "        \"\"\"\n",
    "        # calcula a data da última compra de cada cliente\n",
    "        df_recency = dataframe_purchases.loc[:, ['customer_id', 'invoice_date']].groupby('customer_id').max().reset_index()\n",
    "\n",
    "        # calcula a recência em dias da última compra de cada cliente em relação à data mais recente da base de dados filtrada\n",
    "        df_recency.loc[:, 'recency_days'] = (dataframe_filtered['invoice_date'].max() - df_recency['invoice_date']).dt.days\n",
    "\n",
    "        # retorna o DataFrame apenas com as colunas 'customer_id' e 'recency_days'\n",
    "        return df_recency[['customer_id', 'recency_days']]\n",
    "\n",
    "    def create_quantity_purchased(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula a quantidade de produtos adquiridos por cada cliente.\n",
    "\n",
    "        Args:\n",
    "            dataframe_purchases (pd.DataFrame): DataFrame com as informações de compras de todos os clientes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame com as colunas 'customer_id' e 'qty_products', indicando a quantidade de produtos adquiridos por cada cliente.\n",
    "        \"\"\"\n",
    "        # agrupa as informações de compras por customer_id e conta o número de StockCode para cada grupo\n",
    "        qty_purchased = dataframe_purchases.loc[:, ['customer_id', 'stock_code']].groupby('customer_id').count()\n",
    "\n",
    "        # renomeia a coluna StockCode para qty_products e reseta o índice para transformar o CustomerID em uma coluna\n",
    "        qty_purchased = qty_purchased.reset_index().rename(columns={'stock_code': 'qty_products'})\n",
    "\n",
    "        # retorna o DataFrame com as colunas 'customer_id' e 'qty_products'\n",
    "        return qty_purchased\n",
    "\n",
    "    def create_freq_purchases(dataframe_purchases: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates the purchase frequency of each customer based on the purchase history.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe_purchases : pd.DataFrame\n",
    "            DataFrame with purchase history of each customer, containing columns CustomerID, InvoiceNo, and InvoiceDate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with the purchase frequency of each customer, containing columns CustomerID and frequency.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate time range of purchases for each customer\n",
    "        df_aux = (dataframe_purchases[['customer_id', 'invoice_no', 'invoice_date']]\n",
    "                  .drop_duplicates()\n",
    "                  .groupby('customer_id')\n",
    "                  .agg(max_=('invoice_date', 'max'),\n",
    "                       min_=('invoice_date', 'min'),\n",
    "                       days_=('invoice_date', lambda x: ((x.max() - x.min()).days) + 1),\n",
    "                       buy_=('invoice_no', 'count'))\n",
    "                  .reset_index())\n",
    "\n",
    "        # Calculate frequency of purchases for each customer\n",
    "        df_aux['frequency'] = df_aux[['buy_', 'days_']].apply(\n",
    "            lambda x: x['buy_'] / x['days_'] if x['days_'] != 0 else 0, axis=1)\n",
    "\n",
    "        return df_aux\n",
    "\n",
    "    def create_qty_returns(dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the total quantity of returned products for each customer.\n",
    "\n",
    "        Args:\n",
    "            dataframe_returns: A pandas DataFrame containing information about returns.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the total quantity of returned products for each customer.\n",
    "        \"\"\"\n",
    "        # Validate input data\n",
    "        # if dataframe_returns is None:\n",
    "        #     raise ValueError(\"Input DataFrame is empty\")\n",
    "        # if not all(col in dataframe_returns.columns for col in ['CustomerID', 'Quantity']):\n",
    "        #     raise ValueError(\"Input DataFrame must contain 'CustomerID' and 'Quantity' columns\")\n",
    "\n",
    "        # Compute quantity of returns\n",
    "        df_returns = dataframe_returns[['CustomerID', 'Quantity']].groupby('CustomerID').sum().reset_index().rename(columns={'Quantity': 'qty_returns'})\n",
    "        df_returns['qty_returns'] = df_returns['qty_returns']* -1\n",
    "\n",
    "        return df_returns\n",
    "\n",
    "    def run_feature_engineering(dataframe_filtered: pd.DataFrame, dataframe_purchases: pd.DataFrame, dataframe_returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs feature engineering on the input dataframes and returns a new dataframe with the engineered features.\n",
    "\n",
    "        Args:\n",
    "            dataframe_filtered: A pandas DataFrame containing filtered customer order data.\n",
    "            dataframe_purchases: A pandas DataFrame containing customer purchase data.\n",
    "            dataframe_returns: A pandas DataFrame containing customer return data.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame with the engineered features for each customer.\n",
    "        \"\"\"\n",
    "        # Check if input dataframes are empty\n",
    "        if dataframe_filtered.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_filtered' is empty\")\n",
    "        if dataframe_purchases.empty:\n",
    "            raise ValueError(\"Input DataFrame 'dataframe_purchases' is empty\")\n",
    "        # if dataframe_returns.empty:\n",
    "        #     raise ValueError(\"Input DataFrame 'dataframe_returns' is empty\")\n",
    "\n",
    "        # Check if required columns are present in input dataframes\n",
    "        required_columns = ['CustomerID', 'InvoiceDate', 'StockCode', 'Quantity', 'UnitPrice']\n",
    "        for df, name in zip([dataframe_filtered, dataframe_purchases], ['dataframe_filtered', 'dataframe_purchases']):\n",
    "            missing_columns = set(required_columns) - set(df.columns)\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing columns {missing_columns} in input DataFrame '{name}'\")\n",
    "        if 'CustomerID' not in dataframe_returns.columns:\n",
    "            raise ValueError(\"Column 'customer_id' not found in input DataFrame 'dataframe_returns'\")\n",
    "        if 'Quantity' not in dataframe_returns.columns:\n",
    "            raise ValueError(\"Column 'quantity' not found in input DataFrame 'dataframe_returns'\")\n",
    "\n",
    "        # Perform feature engineering\n",
    "        df_fengi = keep_features(dataframe_filtered, ['customer_id']).drop_duplicates(ignore_index=True)\n",
    "        gross_revenue = calculate_gross_revenue(dataframe_purchases)\n",
    "        df_recency = create_recency(dataframe_purchases, dataframe_filtered)\n",
    "        df_qty_products = create_quantity_purchased(dataframe_purchases)\n",
    "        df_freq = create_freq_purchases(dataframe_purchases)\n",
    "        returns = create_qty_returns(dataframe_returns)\n",
    "\n",
    "        # Merge dataframes\n",
    "        dfs = [df_fengi, gross_revenue, df_recency, df_qty_products, df_freq, returns]\n",
    "        df_fengi = reduce(lambda left,right: pd.merge(left, right, on='CustomerID', how='left'), dfs)\n",
    "\n",
    "        # Fill NaN values\n",
    "        df_fengi['qty_returns'] = df_fengi['qty_returns'].fillna(0)\n",
    "\n",
    "        # Select final features and return dataframe\n",
    "        features = ['customer_id', 'gross_revenue', 'recency_days', 'qty_products', 'frequency', 'qty_returns']\n",
    "        return keep_features(df_fengi, features).dropna()\n",
    "    def table_exists(dataset_table_id: str) -> bool:\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        try:\n",
    "            client.get_table(dataset_table_id)  # Make an API request.\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "    \n",
    "    def save_to_bigquery(dataframe: pd.DataFrame, project_name: str, dataset_table_name: str):\n",
    "        client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Load data to BQ\n",
    "        job = client.load_table_from_dataframe(dataframe, dataset_table_name)\n",
    "        job.result()\n",
    "        \n",
    "    def run_bq_query(sql: str, project_name: str) -> Union[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "        Args:\n",
    "            sql: SQL query, as a string, to execute in BigQuery\n",
    "        Returns:\n",
    "            df: DataFrame of results from query,  or error, if any\n",
    "        \"\"\"\n",
    "        \n",
    "        bq_client = bigquery.Client(project=project_name)\n",
    "\n",
    "        # Try dry run before executing query to catch any errors\n",
    "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "        bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        # If dry run succeeds without errors, proceed to run query\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "        job_id = client_result.job_id\n",
    "        # Wait for query/job to finish running. then get & return data frame\n",
    "        df = client_result.result().to_arrow().to_pandas()\n",
    "        print(f\"Finished job_id: {job_id}\")\n",
    "        return df\n",
    "\n",
    "    logging.info('Carregando as tabelas da preparacao de dados')\n",
    "    query_filtered = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_FILTERED_TEMP_ID}`\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    df_filtered = pd.read_gbq(query=query_filtered, \n",
    "                         project_id=PROJECT_NUMBER)\n",
    "    \n",
    "    query_purchases = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_PURCHASES_TEMP_ID}`\n",
    "                    WHERE InvoiceDate <= CURRENT_TIMESTAMP() \"\"\"\n",
    "    df_purchases = pd.read_gbq(query=query_purchases, \n",
    "                         project_id=PROJECT_NUMBER)\n",
    "    \n",
    "    query_returns = f\"\"\"SELECT *\n",
    "                    FROM  `{PROJECT_ID}.{DATASET_ID}.{TABLE_RETURNS_TEMP_ID}`\"\"\"\n",
    "    df_returns = pd.read_gbq(query=query_returns, \n",
    "                         project_id=PROJECT_NUMBER) \n",
    "    \n",
    "    logging.info('Transformando a coluna InvoiceDate para o tipo DATE')\n",
    "    column_to_date(df_filtered, 'InvoiceDate')\n",
    "    column_to_date(df_purchases, 'InvoiceDate')\n",
    "    \n",
    "    logging.info(f'Iniciando a verificacao de existencia da tabela: {DATASET_ID}.{TABLE_ID}')\n",
    "    # Verifica se a tabela existe\n",
    "    if table_exists(f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"):\n",
    "        logging.info('Tabela existente, inicia insercao de novos dados')\n",
    "        sql_new_customers = f\"\"\"SELECT\n",
    "                                      DISTINCT CustomerID\n",
    "                                    FROM\n",
    "                                      `{PROJECT_ID}.{DATASET_ID}.{TABLE_RAW_ID}`\n",
    "                                    WHERE\n",
    "                                      InvoiceDate = CURRENT_DATE()\"\"\"\n",
    "        new_customers = pd.read_gbq(sql_new_customers, project_id=PROJECT_NUMBER)['CustomerID'].tolist()\n",
    "       \n",
    "        df_fengi = run_feature_engineering(df_filtered.loc[df_filtered['CustomerID'].isin(new_customers)], \n",
    "                                           df_purchases.loc[df_purchases['CustomerID'].isin(new_customers)], \n",
    "                                           df_returns.loc[df_returns['CustomerID'].isin(new_customers)])\n",
    "        \n",
    "        # Inserir os dados na tabela usando SQL\n",
    "        pandas_gbq.to_gbq(df_fengi, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}', project_id=PROJECT_NUMBER, if_exists='append')\n",
    "        sql_update_new_customer = f\"\"\"\n",
    "                                        UPDATE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "                                        SET values = generate_uuid(),\n",
    "                                        timestamp = current_timestamp()\n",
    "                                        WHERE CustomerID IN {tuple(new_customers)}\"\"\"\n",
    "        logging.info(sql_update_new_customer)\n",
    "        run_bq_query(sql_update_new_customer, project_name=PROJECT_ID)\n",
    "    else:\n",
    "        # Cria a tabela e insere os dados\n",
    "        logging.info('Tabela nao existente, cria a tabela e inicia insercao dos dados')\n",
    "        df_fengi = run_feature_engineering(df_filtered, df_purchases, df_returns)\n",
    "        pandas_gbq.to_gbq(df_fengi, f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}', project_id=PROJECT_NUMBER, if_exists='fail')\n",
    "        query = f\"\"\"CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` as (\n",
    "                    SELECT\n",
    "                        *,\n",
    "                        generate_uuid() as values,\n",
    "                        current_timestamp() as timestamp,\n",
    "                    FROM \n",
    "                        `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`);\"\"\"\n",
    "        run_bq_query(query, project_name=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eaeac1-8c0c-4a13-badc-9a1e030fbf7b",
   "metadata": {},
   "source": [
    "# 4.0 Feature Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437692da-fff1-4885-b2ae-e22fd5707401",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"pyarrow\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    "    output_component_file=\"feature_store.yaml\")\n",
    "def create_feature_store():\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import Feature, Featurestore\n",
    "    #https://medium.com/google-cloud/how-do-you-use-feature-store-in-the-mlops-process-on-vertex-ai-802ddca2cac4\n",
    "    #https://www.youtube.com/watch?v=jXD8Sfx4hvQ\n",
    "    \n",
    "    logging.info('Iniciando o componente')\n",
    "    \n",
    "    PROJECT_ID = 'gcpproject'\n",
    "    DATASET_ID = 'gcp_bq'\n",
    "    TABLE_ID = 'dados_engenharia_features'\n",
    "    FEATURESTORE_ID=\"ecommerce_feature_store\"\n",
    "    VALUES_ENTITY_ID = \"values\"\n",
    "    VALUES_BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    FEATURE_TIME = 'timestamp'\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    aiplatform.init(project = project_number, location= REGION)\n",
    "    \n",
    "    try:\n",
    "        # Checks if there is already a Featurestore\n",
    "        ecommerce_feature_store = aiplatform.Featurestore(f\"{FEATURESTORE_ID}\")\n",
    "        logging.info(f\"\"\"A feature store {FEATURESTORE_ID} ja existe.\"\"\")\n",
    "    except:\n",
    "        # Creates a Featurestore\n",
    "        logging.info(f\"\"\"Criando a feature store: {FEATURESTORE_ID}.\"\"\")\n",
    "        ecommerce_feature_store = aiplatform.Featurestore.create(\n",
    "            featurestore_id=f\"{FEATURESTORE_ID}\",\n",
    "            online_store_fixed_node_count=1,\n",
    "            sync=True,\n",
    "        )\n",
    "        \n",
    "    try:\n",
    "        # get entity type, if it already exists\n",
    "        values_entity_type = ecommerce_feature_store.get_entity_type(entity_type_id=VALUES_ENTITY_ID)\n",
    "    except:\n",
    "        # else, create entity type\n",
    "        values_entity_type = ecommerce_feature_store.create_entity_type(\n",
    "            entity_type_id=VALUES_ENTITY_ID, description=\"Values Entity\", sync=True\n",
    "        )\n",
    "    \n",
    "    values_feature_configs = {\n",
    "                                \"gross_revenue\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Gross Revenue\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"recency_days\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Recency Days\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"qty_products\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Quantity products\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"frequency\": {\n",
    "                                    \"value_type\": \"DOUBLE\",\n",
    "                                    \"description\": \"Frequency\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                                },\n",
    "                                \"qty_returns\": {\n",
    "                                    \"value_type\": \"INT64\",\n",
    "                                    \"description\": \"Quantity returns\",\n",
    "                                    \"labels\": {\"status\": \"passed\"},\n",
    "                            }}\n",
    "    values_feature_ids = values_entity_type.batch_create_features(\n",
    "        feature_configs=values_feature_configs, sync=True\n",
    "    )\n",
    "    \n",
    "    VALUES_FEATURES_IDS = [feature.name for feature in values_feature_ids.list_features()]\n",
    "    \n",
    "    logging.info(f\"\"\"Ingerindo os dados na feature store: {FEATURESTORE_ID}.\"\"\")\n",
    "    values_entity_type.ingest_from_bq(\n",
    "                                        feature_ids=VALUES_FEATURES_IDS,\n",
    "                                        feature_time=FEATURE_TIME,\n",
    "                                        bq_source_uri=VALUES_BQ_SOURCE_URI,\n",
    "                                        entity_id_field=VALUES_ENTITY_ID,\n",
    "                                        disable_online_serving=True,\n",
    "                                        worker_count=2,\n",
    "                                        sync=True,\n",
    "                                    )\n",
    "    # enable api: https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=343941956592%22\n",
    "    #https://aiinpractice.com/gcp-mlops-vertex-ai-feature-store/\n",
    "    #https://medium.com/hacking-talent/vertexais-feature-store-for-dummies-3d798b45ece4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
